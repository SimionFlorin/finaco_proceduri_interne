{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "647fc5c5",
      "metadata": {
        "id": "647fc5c5"
      },
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt\n",
        "%pip install -U qdrant_client\n",
        "%pip install git+https://github.com/openai/CLIP.git\n",
        "%pip install llama-index-embeddings-clip\n",
        "%pip install llama-index-embeddings-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e82f521a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "import fitz  # PyMuPDF\n",
        "import docx\n",
        "import PyPDF2\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core.query_engine import SimpleMultiModalQueryEngine\n",
        "import os\n",
        "from typing import List, Dict\n",
        "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
        "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "import qdrant_client\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "# # Load it\n",
        "from llama_index.core import load_index_from_storage\n",
        "from dotenv import load_dotenv\n",
        "import shutil\n",
        "load_dotenv()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7552ccf",
      "metadata": {},
      "outputs": [],
      "source": [
        "from docx2pdf import convert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7ffed7de",
      "metadata": {},
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
        "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e648a24e",
      "metadata": {
        "id": "e648a24e"
      },
      "outputs": [],
      "source": [
        "\n",
        "openai_mm_llm = OpenAIMultiModal(\n",
        "    model=\"gpt-4o\", api_key=OPENAI_API_KEY, max_new_tokens=1500\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9090fcc2",
      "metadata": {},
      "outputs": [],
      "source": [
        "input_folder = \"Proceduri_Interne\"\n",
        "image_directory = \"./iframe_image_extraction\"\n",
        "pdf_directory = \"pdfs\"\n",
        "output_folder = \"iframe_image_extraction\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5855b69f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def extract_images_from_word(doc_path, output_folder):\n",
        "    doc = Document(doc_path)\n",
        "    word_image_list = []\n",
        "    for i, rel in enumerate(doc.part.rels.values()):\n",
        "        if \"image\" in rel.target_ref:\n",
        "            img = rel.target_part.blob\n",
        "            img_filename = os.path.join(output_folder, f\"{os.path.basename(doc_path)}_image_{i+1}.png\")\n",
        "            with open(img_filename, \"wb\") as f:\n",
        "                f.write(img)\n",
        "            word_image_list.append({\"image\": img_filename, \"text\": \"\", \"origin\": doc_path})\n",
        "    \n",
        "    return word_image_list\n",
        "\n",
        "def extract_images_from_pdf(pdf_path, output_folder):\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "    pdf_image_list = []\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        image_list = page.get_images(full=True)\n",
        "        for img_index, img in enumerate(image_list):\n",
        "            xref = img[0]\n",
        "            base_image = pdf_document.extract_image(xref)\n",
        "            img_bytes = base_image[\"image\"]\n",
        "            img_filename = os.path.join(output_folder, f\"{os.path.basename(pdf_path)}_page_{page_num+1}_image_{img_index+1}.png\")\n",
        "            with open(img_filename, \"wb\") as f:\n",
        "                f.write(img_bytes)\n",
        "            pdf_image_list.append({\"image\": img_filename, \"text\": \"\", \"origin\": pdf_path})\n",
        "            \n",
        "    return pdf_image_list\n",
        "\n",
        "def extract_images_from_folder(folder_path, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    word_image_list = []\n",
        "    pdf_image_list = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        # if filename.endswith(\".docx\"):\n",
        "        #     word_image_list = word_image_list + extract_images_from_word(file_path, output_folder)\n",
        "        # elif filename.endswith(\".pdf\"):\n",
        "        #     pdf_image_list = pdf_image_list + extract_images_from_pdf(file_path, output_folder)\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            pdf_image_list = pdf_image_list + extract_images_from_pdf(file_path, output_folder)\n",
        "            \n",
        "    return word_image_list+pdf_image_list\n",
        "# Specify the folder containing the Word and PDF files and the output folder\n",
        "\n",
        "\n",
        "# Put your local directory here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8644f3f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def is_nonsense_text(text):\n",
        "    # Remove all whitespace\n",
        "    cleaned_text = ''.join(text.split())\n",
        "    # Check if the cleaned text consists only of digits\n",
        "    if cleaned_text.isdigit():\n",
        "        return True\n",
        "    # Check if the text matches the pattern of numbers separated by newlines\n",
        "    if re.match(r'^(\\d+\\n?)+$', text.strip()):\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d237020",
      "metadata": {
        "id": "7d237020"
      },
      "source": [
        "## Generate image reasoning from GPT4V Multi-Modal LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1f569a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "image_list = extract_images_from_folder(pdf_directory, output_folder)\n",
        "image_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5a64bb6",
      "metadata": {
        "id": "e5a64bb6",
        "outputId": "600e97be-a079-4962-fd49-b3c58f631d29"
      },
      "outputs": [],
      "source": [
        "\n",
        "image_paths = []\n",
        "for img_path in os.listdir(\"./iframe_image_extraction\"):\n",
        "    if \".pdf\" in img_path: \n",
        "        image_paths.append(str(os.path.join(\"./iframe_image_extraction\", img_path)))\n",
        "\n",
        "\n",
        "def plot_images(image_paths):\n",
        "    images_shown = 0\n",
        "    plt.figure(figsize=(16, 9))\n",
        "    for img_path in image_paths[:6]:\n",
        "        if os.path.isfile(img_path):\n",
        "            image = Image.open(img_path)\n",
        "\n",
        "            plt.subplot(2, 3, images_shown + 1)\n",
        "            plt.imshow(image)\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "\n",
        "            images_shown += 1\n",
        "            if images_shown >= 9:\n",
        "                break\n",
        "\n",
        "\n",
        "plot_images(image_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e54694f",
      "metadata": {
        "id": "4e54694f"
      },
      "source": [
        "### Using GPT4V to understand those input images "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b55d6f36",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def process_images(directory):\n",
        "    results = []\n",
        "    \n",
        "    image_documents = SimpleDirectoryReader(directory).load_data()\n",
        "    \n",
        "    print(image_documents)\n",
        "    for image_document in image_documents:\n",
        "            \n",
        "            # Generate description\n",
        "            response = openai_mm_llm.complete(\n",
        "                prompt=\"Descrie imaginea de mai jos ca text alternativ:\",\n",
        "                image_documents=[image_document],\n",
        "            )\n",
        "            \n",
        "            # Add result to the list\n",
        "            result = {\n",
        "                \"filename\": image_document.metadata[\"file_path\"],\n",
        "                \"open_ai_description\": response.text\n",
        "            }\n",
        "            print(result)\n",
        "            results.append(result)\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Process all images and get results\n",
        "image_descriptions = process_images(image_directory)\n",
        "\n",
        "# Print results\n",
        "for item in image_descriptions:\n",
        "    print(f\"Filename: {item['filename']}\")\n",
        "    print(f\"Description: {item['open_ai_description']}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d2c82f",
      "metadata": {},
      "outputs": [],
      "source": [
        "image_descriptions_df = pd.DataFrame(image_descriptions)\n",
        "image_descriptions_df[\"filename\"] = image_descriptions_df[\"filename\"].apply(lambda x: os.path.basename(x))\n",
        "image_descriptions_df.to_csv(\"image_descriptions.csv\", index=False)\n",
        "image_descriptions_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecedb0e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "image_df_origin_df = pd.DataFrame(image_list)\n",
        "image_df_origin_df[\"image\"] = image_df_origin_df[\"image\"].apply(lambda x: os.path.basename(x))\n",
        "image_df_origin_df\n",
        "image_descriptions_df_2 = pd.merge(\n",
        "    image_descriptions_df, image_df_origin_df, left_on=\"filename\", right_on=\"image\", how=\"left\"\n",
        "    ).drop(columns=[\"filename\",\"text\"]).rename(columns={\"image\": \"image_path\",\"origin\":\"origin_filename\"})\n",
        "image_descriptions_df_2[\"origin_filename\"] = image_descriptions_df_2[\"origin_filename\"].apply(lambda x: os.path.basename(x))\n",
        "image_descriptions_df_2.to_csv(\"image_descriptions.csv\", index=False)\n",
        "image_descriptions_df_2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0f826d1",
      "metadata": {},
      "source": [
        "### Image description already extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e357908b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    \"\"\"\n",
        "    Extract all text from a .docx file.\n",
        "    \n",
        "    :param file_path: str, path to the .docx file\n",
        "    :return: str, all text content from the document\n",
        "    \"\"\"\n",
        "    doc = docx.Document(file_path)\n",
        "    full_text = []\n",
        "    \n",
        "    for para in doc.paragraphs:\n",
        "        full_text.append(para.text)\n",
        "    \n",
        "    for table in doc.tables:\n",
        "        for row in table.rows:\n",
        "            for cell in row.cells:\n",
        "                full_text.append(cell.text)\n",
        "    \n",
        "    return '\\n'.join(full_text)\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    \"\"\"\n",
        "    Extract all text from a PDF file.\n",
        "    \n",
        "    :param file_path: str, path to the PDF file\n",
        "    :return: str, all text content from the document\n",
        "    \"\"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        num_pages = len(reader.pages)\n",
        "        \n",
        "        full_text = []\n",
        "        for page_num in range(num_pages):\n",
        "            page = reader.pages[page_num]\n",
        "            full_text.append(page.extract_text())\n",
        "    \n",
        "    return '\\n'.join(full_text)\n",
        "\n",
        "\n",
        "def process_folder(folder_path: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Process all supported files (.docx and .pdf) in a folder and extract text from them.\n",
        "    \n",
        "    :param folder_path: str, path to the folder containing the files\n",
        "    :return: List[Dict[str, str]], a list of dictionaries containing filename and extracted raw text\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        \n",
        "        if filename.endswith('.docx'):\n",
        "            extracted_text = extract_text_from_docx(file_path)\n",
        "        elif filename.endswith('.pdf'):\n",
        "            extracted_text = extract_text_from_pdf(file_path)\n",
        "        else:\n",
        "            continue  # Skip unsupported file types\n",
        "        \n",
        "        results.append({\n",
        "            'origin_filename': filename,\n",
        "            'extracted_raw_text': extracted_text\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "extracted_text = process_folder(input_folder)\n",
        "extracted_text_df = pd.DataFrame(extracted_text)\n",
        "extracted_text_df.to_csv(\"extracted_text.csv\", index=False)\n",
        "extracted_text_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d89364a",
      "metadata": {
        "id": "1d89364a"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import requests\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "image_descriptions_df = pd.read_csv(\"image_descriptions.csv\")\n",
        "extracted_text_df = pd.read_csv(\"extracted_text.csv\")\n",
        "\n",
        "merged_df = pd.merge(\n",
        "    extracted_text_df, image_descriptions_df, left_on=\"origin_filename\", right_on=\"origin_filename\", how=\"outer\"\n",
        ")\n",
        "merged_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec1ae739",
      "metadata": {},
      "outputs": [],
      "source": [
        "filename_image_descriptions_summed = image_descriptions_df.groupby(\"origin_filename\").agg(\n",
        "    image_descriptions_summed=(\"open_ai_description\", lambda x: ' '.join(x))\n",
        ").reset_index()\n",
        "merged_df = pd.merge(\n",
        "    merged_df, filename_image_descriptions_summed, left_on=\"origin_filename\", right_on=\"origin_filename\", how=\"left\"\n",
        ")\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d223a4d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df[\"extracted_raw_text\"] = merged_df[\"extracted_raw_text\"].fillna(\"\")\n",
        "merged_df[\"open_ai_description\"] = merged_df[\"open_ai_description\"].fillna(\"\")\n",
        "merged_df[\"image_descriptions_summed\"] = merged_df[\"image_descriptions_summed\"].fillna(\"\")\n",
        "merged_df[\"extracted_raw_text_length\"] = merged_df[\"extracted_raw_text\"].apply(len)\n",
        "merged_df[\"open_ai_description_length\"] = merged_df[\"open_ai_description\"].apply(len)\n",
        "merged_df[\"image_descriptions_summed_length\"] = merged_df[\"image_descriptions_summed\"].apply(len)\n",
        "\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24a35315",
      "metadata": {},
      "outputs": [],
      "source": [
        "merged_df.groupby(\"origin_filename\")[[\"extracted_raw_text_length\",\"image_descriptions_summed_length\"]].mean().reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f476d8a",
      "metadata": {
        "id": "4f476d8a"
      },
      "source": [
        "### Build Multi-modal index and vector store to index both text and images (locally)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a748dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "extracted_text_df[\"extracted_raw_text\"] = extracted_text_df[\"extracted_raw_text\"].fillna(\"\")\n",
        "extracted_text_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab8ed2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(extracted_text_df.shape)\n",
        "\n",
        "import pandas as pd\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "\n",
        "# Assuming you have your DataFrame loaded as 'df'\n",
        "# with a column named 'extracted_text_df'\n",
        "\n",
        "# Initialize the SentenceSplitter with overlap\n",
        "splitter = TokenTextSplitter(\n",
        "    chunk_size=4000,\n",
        "    chunk_overlap=400,\n",
        ")    \n",
        "    \n",
        "def chunk_text(text):    \n",
        "    print(len(text))\n",
        "    \n",
        "    splitter = TokenTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
        "    chunks = splitter.split_text(text)\n",
        "    print(len(chunks))\n",
        "    return chunks\n",
        "\n",
        "# Apply the chunking function to the column\n",
        "extracted_text_df['chunked_text'] = extracted_text_df['extracted_raw_text'].apply(chunk_text)\n",
        "\n",
        "# Explode the DataFrame to create one row per chunk\n",
        "extracted_text_df_exploded = extracted_text_df.explode('chunked_text')\n",
        "\n",
        "# Reset the index of the exploded DataFrame\n",
        "extracted_text_df_exploded = extracted_text_df_exploded.reset_index(drop=True)\n",
        "\n",
        "# Now df_exploded contains the chunked text, with each chunk in its own row\n",
        "extracted_text_df_exploded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64180159",
      "metadata": {},
      "outputs": [],
      "source": [
        "image_descriptions_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6f5c91",
      "metadata": {},
      "source": [
        "### Uploading text in hosted Qdrant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e8caba5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "import openai\n",
        "from qdrant_client.http import models\n",
        "\n",
        "openai_client = openai.Client(\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        ")\n",
        "\n",
        "# embedding_model_name = \"text-embedding-3-small\"\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=QDRANT_URL, \n",
        "    api_key=QDRANT_API_KEY\n",
        ")\n",
        "\n",
        "print(qdrant_client.get_collections())\n",
        "\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "collection_name = \"finaco_proceduri_interne_text_only\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e93ecea",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Create a new collection (if it doesn't exist)\n",
        "qdrant_client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ed56cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(image_descriptions_df.columns)\n",
        "print(extracted_text_df_exploded.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14a00465",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "embedding_model = OpenAIEmbedding()\n",
        "embedding_model_name = embedding_model.model_name\n",
        "embedding_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "406dd3ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import Document, VectorStoreIndex, StorageContext\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "\n",
        "# Step 1: Prepare the Documents\n",
        "documents = []\n",
        "\n",
        "for index, row in image_descriptions_df.iterrows():\n",
        "    doc = Document(\n",
        "        text=row['open_ai_description'],\n",
        "        metadata={\n",
        "            'image_path': row['image_path'],\n",
        "            'origin_filename': row['origin_filename']\n",
        "        }\n",
        "    )\n",
        "    documents.append(doc)\n",
        "\n",
        "# Step 2: Set Up the Qdrant Vector Store\n",
        "vector_store = QdrantVectorStore(client=qdrant_client, collection_name=collection_name)\n",
        "\n",
        "# Step 3: Create the Storage and Service Contexts\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Step 4: Create the Index and Insert Documents\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    storage_context=storage_context,\n",
        ")\n",
        "\n",
        "# Step 5: Verify the Data Insertion (Optional)\n",
        "retriever = index.as_retriever(similarity_top_k=5)\n",
        "response = retriever.retrieve(\"Cum declari reprezentantul Fiscal?\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e53c9b9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core import Document, VectorStoreIndex, StorageContext\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "\n",
        "# Step 1: Prepare the Documents\n",
        "documents = []\n",
        "\n",
        "for index, row in extracted_text_df_exploded.iterrows():\n",
        "    doc = Document(\n",
        "        text=row['chunked_text'],\n",
        "        metadata={\n",
        "            'origin_filename': row['origin_filename']\n",
        "        }\n",
        "    )\n",
        "    documents.append(doc)\n",
        "\n",
        "# Step 2: Set Up the Qdrant Vector Store\n",
        "vector_store = QdrantVectorStore(client=qdrant_client, collection_name=collection_name)\n",
        "\n",
        "# Step 3: Create the Storage and Service Contexts\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Step 4: Create the Index and Insert Documents\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    storage_context=storage_context,\n",
        ")\n",
        "\n",
        "# Step 5: Verify the Data Insertion (Optional)\n",
        "retriever = index.as_retriever(similarity_top_k=5)\n",
        "response = retriever.retrieve(\"Cum declari reprezentantul Fiscal?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c16ad4d",
      "metadata": {},
      "source": [
        "### Retrieve data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08240a7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "import os\n",
        "import openai\n",
        "from qdrant_client.http import models\n",
        "# from llama_index import VectorStoreIndex, StorageContext \n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import StorageContext\n",
        "# from llama_index.vector_stores import QdrantVectorStore\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "openai_client = openai.Client(\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        ")\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=QDRANT_URL, \n",
        "    api_key=QDRANT_API_KEY\n",
        ")\n",
        "\n",
        "collection_name = \"finaco_proceduri_interne_text_only\"\n",
        "\n",
        "vector_store = QdrantVectorStore(client=qdrant_client, collection_name=collection_name)\n",
        "\n",
        "print(qdrant_client.get_collections())\n",
        "\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "embed_model = OpenAIEmbedding(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=embed_model)\n",
        "index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f442b536",
      "metadata": {},
      "outputs": [],
      "source": [
        "query_text = \"Cum declari reprezentantul Fiscal?\"\n",
        "query_vector = openai_client.embeddings.create(input=query_text, model=embedding_model_name).data[0].embedding\n",
        "\n",
        "qdrant_client.search(\n",
        "    collection_name=collection_name,\n",
        "    query_vector=query_vector,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60618aad",
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = index.as_retriever(similarity_top_k=5)\n",
        "response = retriever.retrieve(\"Cum declari reprezentantul Fiscal?\")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d585e054",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now you can perform string-based queries\n",
        "query_str = \"Cum declari reprezentantul Fiscal?\"\n",
        "# set Logging to DEBUG for more detailed outputs\n",
        "llm  = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "chat_engine = index.as_chat_engine(llm=llm)\n",
        "response = chat_engine.chat(\"Cum declari reprezentantul Fiscal?\")\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25121620-a39a-4714-bb6a-c03b7a9b937d",
      "metadata": {
        "id": "25121620-a39a-4714-bb6a-c03b7a9b937d"
      },
      "source": [
        "### 2. Multi-Modal RAG Querying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "21a30e2c-785f-49ac-8131-6a4772e2cdf8",
      "metadata": {
        "id": "21a30e2c-785f-49ac-8131-6a4772e2cdf8"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core.query_engine import SimpleMultiModalQueryEngine\n",
        "\n",
        "qa_tmpl_str = (\n",
        "    \"Informațiile de context sunt mai jos.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Având în vedere informațiile din context și nu cunoștințe anterioare, \"\n",
        "    \"răspunde la întrebare.\\n\"\n",
        "    \"Întrebare: {query_str}\\n\"\n",
        "    \"Răspuns: \"\n",
        ")\n",
        "\n",
        "qa_tmpl = PromptTemplate(qa_tmpl_str)\n",
        "llm  = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "query_engine = index.as_query_engine(\n",
        "    llm=llm, text_qa_template=qa_tmpl, similarity_top_k=5, \n",
        ")\n",
        "\n",
        "query_str = \"Cum declari reprezentantul Fiscal?\"\n",
        "response = query_engine.query(query_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2571bcbd-21cb-4cc5-90ed-0f621f765ccc",
      "metadata": {
        "id": "2571bcbd-21cb-4cc5-90ed-0f621f765ccc",
        "outputId": "47a50c37-dcd0-4c85-9d42-e104e6cb4f0e"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81278f47",
      "metadata": {},
      "outputs": [],
      "source": [
        "source_nodes = response.source_nodes\n",
        "source_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2fb8393",
      "metadata": {},
      "outputs": [],
      "source": [
        "images = []\n",
        "for node in source_nodes:\n",
        "    if \"image_path\" in node.metadata:\n",
        "        print(node.metadata[\"image_path\"])\n",
        "        images.append(\"./iframe_image_extraction/\"+node.metadata[\"image_path\"])\n",
        "\n",
        "print(images)\n",
        "\n",
        "plot_images(images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3576e20c",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_images(images)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
